BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters
https://github.com/google-research/bert#pre-trained-models
https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip

BERT-Base multilingual
https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip

